{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from astropy.table import Table\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found gaia_mock_streams/stream-9549.fits.gz\n",
      "File not found gaia_mock_streams/stream-3649.fits.gz\n",
      "File not found gaia_mock_streams/stream-5185.fits.gz\n",
      "File not found gaia_mock_streams/stream-1011.fits.gz\n",
      "File not found gaia_mock_streams/stream-112.fits.gz\n",
      "File not found gaia_mock_streams/stream-6739.fits.gz\n",
      "File not found gaia_mock_streams/stream-4624.fits.gz\n",
      "File not found gaia_mock_streams/stream-5698.fits.gz\n",
      "File not found gaia_mock_streams/stream-9391.fits.gz\n",
      "File not found gaia_mock_streams/stream-6468.fits.gz\n",
      "File not found gaia_mock_streams/stream-8566.fits.gz\n",
      "File not found gaia_mock_streams/stream-5204.fits.gz\n",
      "File not found gaia_mock_streams/stream-3055.fits.gz\n",
      "File not found gaia_mock_streams/stream-6869.fits.gz\n",
      "File not found gaia_mock_streams/stream-8947.fits.gz\n",
      "File not found gaia_mock_streams/stream-8516.fits.gz\n",
      "File not found gaia_mock_streams/stream-3704.fits.gz\n",
      "File not found gaia_mock_streams/stream-1333.fits.gz\n",
      "File not found gaia_mock_streams/stream-2100.fits.gz\n",
      "File not found gaia_mock_streams/stream-5743.fits.gz\n",
      "File not found gaia_mock_streams/stream-6728.fits.gz\n",
      "File not found gaia_mock_streams/stream-2325.fits.gz\n",
      "File not found gaia_mock_streams/stream-5485.fits.gz\n",
      "File not found gaia_mock_streams/stream-8187.fits.gz\n",
      "File not found gaia_mock_streams/stream-2451.fits.gz\n",
      "File not found gaia_mock_streams/stream-3076.fits.gz\n",
      "File not found gaia_mock_streams/stream-2754.fits.gz\n",
      "File not found gaia_mock_streams/stream-395.fits.gz\n",
      "File not found gaia_mock_streams/stream-2717.fits.gz\n",
      "File not found gaia_mock_streams/stream-8336.fits.gz\n",
      "File not found gaia_mock_streams/stream-575.fits.gz\n",
      "File not found gaia_mock_streams/stream-2636.fits.gz\n",
      "File not found gaia_mock_streams/stream-3041.fits.gz\n",
      "File not found gaia_mock_streams/stream-1427.fits.gz\n",
      "File not found gaia_mock_streams/stream-6830.fits.gz\n",
      "File not found gaia_mock_streams/stream-5851.fits.gz\n",
      "30 17 9\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "np.random.seed(99)\n",
    "cuts = pd.read_csv(\"ClusterBusters/cuts/cuts.csv\")\n",
    "cuts2 = pd.read_csv(\"ClusterBusters/cuts/cuts2.csv\")\n",
    "cuts3 = pd.read_csv(\"ClusterBusters/cuts/cuts3.csv\")\n",
    "cuts = pd.concat((cuts, cuts2), axis=0, ignore_index=True)\n",
    "cuts = pd.concat((cuts, cuts3), axis=0, ignore_index=True)\n",
    "streams_test = ['gaia_mock_streams/stream-8495.fits.gz',\n",
    " 'gaia_mock_streams/stream-2471.fits.gz',\n",
    " 'gaia_mock_streams/stream-4990.fits.gz',\n",
    " 'gaia_mock_streams/stream-3775.fits.gz',\n",
    " 'gaia_mock_streams/stream-5797.fits.gz',\n",
    " 'gaia_mock_streams/stream-1324.fits.gz',\n",
    " 'gaia_mock_streams/stream-9420.fits.gz',\n",
    " 'gaia_mock_streams/stream-3696.fits.gz',\n",
    " 'gaia_mock_streams/stream-9833.fits.gz']\n",
    "\n",
    "\n",
    "train =  []\n",
    "val = []\n",
    "test = []\n",
    "streams = []\n",
    "tr, v, te = 0,0,0\n",
    "for i in range(len(cuts)):\n",
    "    file_name = cuts.loc[i].stream_file\n",
    "    is_test = file_name in streams_test\n",
    "    is_train = False\n",
    "    is_val = False\n",
    "    if not is_test:\n",
    "        if len(streams) < 30:\n",
    "            is_train = True\n",
    "        else:\n",
    "            is_val = True\n",
    "    try:\n",
    "        table = Table.read(file_name, format='fits')\n",
    "\n",
    "        stream = table.to_pandas()\n",
    "        stream = stream.query('ra > ' + str(cuts.loc[i].ra_min) + ' & ra < ' + str(cuts.loc[i].ra_max) + ' & dec > ' + str(cuts.loc[i].dec_min) + ' & dec < ' + str(cuts.loc[i].dec_max))\n",
    "        stream_name = file_name.replace(\".fits.gz\", \"\").replace(\"gaia_mock_streams\", \"\")\n",
    "        stream[\"g\"] = stream[\"phot_g_mean_mag\"]\n",
    "        stream[\"g_bp\"] = stream[\"phot_g_mean_mag\"] - stream[\"phot_bp_mean_mag\"]\n",
    "        stream[\"g_rp\"] = stream[\"phot_g_mean_mag\"] - stream[\"phot_rp_mean_mag\"]\n",
    "        #print(stream[[\"g\", \"g_bp\", \"g_rp\"]])\n",
    "        #print(stream_name)\n",
    "        noise = pd.read_csv(f\"noise_points/{stream_name}_mul_400_total_noise.csv\", index_col=None)\n",
    "        #print(noise.columns)\n",
    "        noise[\"g\"] = noise[\"phot_g_mean_mag\"]\n",
    "        noise[\"g_bp\"] = noise[\"phot_g_mean_mag\"] - noise[\"phot_bp_mean_mag\"]\n",
    "        noise[\"g_rp\"] = noise[\"phot_g_mean_mag\"] - noise[\"phot_rp_mean_mag\"]\n",
    "        #print(\"through\")\n",
    "        stream[\"is_stream_actual\"] = 1\n",
    "        noise[\"is_stream_actual\"] = 0\n",
    "        arr = np.zeros(len(stream))\n",
    "        arr[np.random.choice(np.arange(len(stream)), replace=False, size = int(0.2*len(stream)))] = True\n",
    "        stream[\"is_train\"] = arr\n",
    "\n",
    "        n_train = int(np.sum(stream[\"is_train\"]))\n",
    "        arr = np.zeros(n_train)\n",
    "        arr[np.random.choice(np.arange(n_train) , replace=False, size = n_train // 2)] = True\n",
    "        training_stream = stream[stream[\"is_train\"] == 1].copy()\n",
    "\n",
    "        training_stream[\"is_reference\"] = arr\n",
    "        k = 1 if is_train else 255\n",
    "        stream_noise = stream[stream[\"is_train\"] == 0]\n",
    "        noise_with_stream_noise = pd.concat((noise, stream_noise), sort=False, ignore_index=True)\n",
    "        final_noise = noise_with_stream_noise.sample(len(training_stream[training_stream.is_train + training_stream.is_reference == 1])*k)\n",
    "        final_noise[\"label\"] = 0\n",
    "        training_stream[\"label\"] = 1\n",
    "        reference_points = training_stream[training_stream.is_reference == 1].copy()\n",
    "        training_dataset = pd.concat((final_noise, training_stream[training_stream.is_reference == 0].copy()), sort=False, ignore_index = True)\n",
    "        features = [\"ra\", \"dec\", \"pmra\", \"pmdec\", \"g_bp\", \"g_rp\", \"g\"]\n",
    "        reference_points = reference_points[features].to_numpy()\n",
    "        X = training_dataset[features].to_numpy()\n",
    "        y = training_dataset[\"label\"].to_numpy()\n",
    "        if reference_points.shape[0] > 2:\n",
    "            if is_train:\n",
    "                tr += 1\n",
    "                for i in range(X.shape[0]):\n",
    "                    train.append((X[i].copy(), y[i].copy(), reference_points.copy()))\n",
    "                streams.append(stream_name)\n",
    "            elif is_val:\n",
    "                v += 1\n",
    "                for i in range(X.shape[0]):\n",
    "                    val.append((X[i].copy(), y[i].copy(), reference_points.copy()))\n",
    "                streams.append(stream_name)\n",
    "            elif is_test:\n",
    "                te += 1\n",
    "                for i in range(X.shape[0]):\n",
    "                    test.append((X[i].copy(), y[i].copy(), reference_points.copy()))\n",
    "        \n",
    "    except:\n",
    "        print(f\"File not found {file_name}\")\n",
    "            \n",
    "        \n",
    "print(tr, v, te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4330\n",
      "430848\n",
      "46848\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train, open(\"train_set.pkl\", \"wb\"))\n",
    "pickle.dump(test, open(\"test_set.pkl\", \"wb\"))\n",
    "pickle.dump(streams, open(\"streams.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open(\"train_set.pkl\", \"rb\"))\n",
    "test = pickle.load(open(\"test_set.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/kae358/miniconda2/envs/NLP/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/scratch/kae358/miniconda2/envs/NLP/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/scratch/kae358/miniconda2/envs/NLP/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/scratch/kae358/miniconda2/envs/NLP/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/scratch/kae358/miniconda2/envs/NLP/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/scratch/kae358/miniconda2/envs/NLP/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import copy\n",
    "device = \"cuda\"\n",
    "num_epochs = 100 # number epoch to train\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def test_model(loader, model):\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    prediction_tresh = []\n",
    "    truth = []\n",
    "    for x, y, ref in loader:\n",
    "        \n",
    "        outputs = F.softmax(model(torch.FloatTensor(x).to(device), torch.FloatTensor(ref).to(device)), dim=0)\n",
    "        predicted = outputs.max(0, keepdim=True)[1]\n",
    "        truth.append(y)\n",
    "        prediction_tresh.append(outputs[1].item())\n",
    "        total += 1\n",
    "        correct += predicted.item() == y\n",
    "    \n",
    "    acc = (100 * correct / total)\n",
    "    auc = metrics.roc_auc_score(truth, prediction_tresh)\n",
    "    prec, rec, f1, _ = metrics.precision_recall_fscore_support(truth, np.array(prediction_tresh) > 0.5, average=\"binary\")\n",
    "    \n",
    "    return acc, auc, prec, rec, f1\n",
    "\n",
    "def train_model(model, optimizer, criterion, num_epochs):\n",
    "    best_val_auc = 0\n",
    "    model_sd = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_t = 0\n",
    "        size = len(train)\n",
    "        for i, (x, y, ref) in enumerate(np.random.permutation(train)):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(torch.FloatTensor(x).to(device), torch.FloatTensor(ref).to(device))\n",
    "            loss = criterion(outputs.unsqueeze(0), torch.LongTensor([y]).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_t += loss.item()\n",
    "            if i % 2000 == 0:\n",
    "                print(f\"Train loss {i}/{size}: {loss_t/(i+1)}\")\n",
    "        train_acc, t_auc, _, _, _ = test_model(train, model)\n",
    "        val_acc, val_auc, val_prec, val_rec, val_f1 = test_model(test, model)\n",
    "        print('Epoch: [{}/{}],  Train acc: {}, Validation Acc: {}, Val auc: {}, Val precision: {}, Val recall: {}, Val f1: {}'.format( \n",
    "                       epoch+1, num_epochs,  train_acc, val_acc, val_auc, val_prec, val_rec, val_f1))\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            model_sd = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "    return best_val_auc, model_sd\n",
    "            \n",
    "    \n",
    "def hyperparameter_search(n_tries, num_epochs, results_name):\n",
    "    n_features = 7\n",
    "    n_hidden = (2,15)\n",
    "    hidden_size = (5, 100)\n",
    "    dropout = (0.1, 0.3)\n",
    "    learning_rates = (-5, -3)\n",
    "    df = pd.DataFrame(columns=[\"n_features\", \"n_hidden\", \"hidden_size\", \"dropout\", \"learning_rate\", \"val_acc\"])\n",
    "    for i in tqdm(range(n_tries)):\n",
    "        options = {\"n_features\": 7,\n",
    "                   \"n_hidden\": np.random.randint(n_hidden[0], n_hidden[1] + 1),\n",
    "                   \"hidden_size\": np.random.randint(hidden_size[0], hidden_size[1] + 1),\n",
    "                   \"dropout\": np.random.uniform(dropout[0], dropout[1])}\n",
    "        print(options)\n",
    "        model = models.SeparateEmbeddings(options).to(\"cuda\")\n",
    "        lr = 10**np.random.uniform(learning_rates[0], learning_rates[1])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        val_auc, model = train_model(model, optimizer, criterion, num_epochs)\n",
    "        options[\"learning_rate\"] = lr\n",
    "        options[\"val_auc\"]  = val_auc\n",
    "        df = df.append(options, ignore_index=True)\n",
    "        df.to_csv(results_name)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"results_2.csv\")\n",
    "df.sort_values(\"val_auc\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"n_features\": 7, \"n_hidden\": 9, \"hidden_size\": 50, \"dropout\": 0.1}\n",
    "#model = models.SeparateEmbeddings(options)\n",
    "#model.to(\"cuda\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0/6064: 0.12039840221405029\n",
      "Train loss 2000/6064: 0.17143114620837374\n",
      "Train loss 4000/6064: 0.17400520192298613\n",
      "Train loss 6000/6064: 0.17500173900548943\n",
      "Epoch: [1/10],  Train acc: 94.11279683377309, Validation Acc: 87.57861635220125, Val auc: 0.9399625981742986, Val precision: 0.8624873609706775, Val recall: 0.8941299790356394, Val f1: 0.8780236747297994\n",
      "Train loss 0/6064: 0.0011713504791259766\n",
      "Train loss 2000/6064: 0.17603273444745257\n",
      "Train loss 4000/6064: 0.16934920619857577\n",
      "Train loss 6000/6064: 0.1736362089297907\n",
      "Epoch: [2/10],  Train acc: 94.32717678100263, Validation Acc: 86.0587002096436, Val auc: 0.93210206171521, Val precision: 0.8909090909090909, Val recall: 0.8218029350104822, Val f1: 0.8549618320610688\n",
      "Train loss 0/6064: 0.06677889823913574\n",
      "Train loss 2000/6064: 0.1664501957330985\n",
      "Train loss 4000/6064: 0.16313786542853603\n",
      "Train loss 6000/6064: 0.16802717727177383\n",
      "Epoch: [3/10],  Train acc: 94.04683377308707, Validation Acc: 85.42976939203355, Val auc: 0.9325272822365499, Val precision: 0.8658008658008658, Val recall: 0.8385744234800838, Val f1: 0.8519701810436634\n",
      "Train loss 0/6064: 0.022163867950439453\n",
      "Train loss 2000/6064: 0.162196422028339\n",
      "Train loss 4000/6064: 0.1654618289642887\n",
      "Train loss 6000/6064: 0.1650667424510666\n",
      "Epoch: [4/10],  Train acc: 93.63456464379948, Validation Acc: 85.63941299790356, Val auc: 0.9267280214829758, Val precision: 0.8935185185185185, Val recall: 0.8092243186582809, Val f1: 0.8492849284928493\n",
      "Train loss 0/6064: 1.9917495250701904\n",
      "Train loss 2000/6064: 0.16030607699811728\n",
      "Train loss 4000/6064: 0.16188945754025466\n",
      "Train loss 6000/6064: 0.1628434872144541\n",
      "Epoch: [5/10],  Train acc: 95.26715039577836, Validation Acc: 88.31236897274633, Val auc: 0.947658320477829, Val precision: 0.8934337997847147, Val recall: 0.870020964360587, Val f1: 0.8815719596388741\n",
      "Train loss 0/6064: 0.06227540969848633\n",
      "Train loss 2000/6064: 0.15583085543152572\n",
      "Train loss 4000/6064: 0.15522910606500598\n",
      "Train loss 6000/6064: 0.15790024458358853\n",
      "Epoch: [6/10],  Train acc: 94.65699208443272, Validation Acc: 86.58280922431865, Val auc: 0.9389973366032462, Val precision: 0.8956916099773242, Val recall: 0.8280922431865828, Val f1: 0.8605664488017428\n",
      "Train loss 0/6064: 0.005630970001220703\n",
      "Train loss 2000/6064: 0.14869331282892564\n",
      "Train loss 4000/6064: 0.15696531789685989\n",
      "Train loss 6000/6064: 0.15678355836745123\n",
      "Epoch: [7/10],  Train acc: 95.15171503957784, Validation Acc: 86.21593291404612, Val auc: 0.9338441473394601, Val precision: 0.8633017875920084, Val recall: 0.860587002096436, Val f1: 0.8619422572178477\n",
      "Train loss 0/6064: 0.10074818134307861\n",
      "Train loss 2000/6064: 0.16806431548825387\n",
      "Train loss 4000/6064: 0.15163769028658153\n",
      "Train loss 6000/6064: 0.1529386165668082\n",
      "Epoch: [8/10],  Train acc: 95.15171503957784, Validation Acc: 87.68343815513627, Val auc: 0.9414986661041009, Val precision: 0.8903365906623235, Val recall: 0.859538784067086, Val f1: 0.8746666666666666\n",
      "Train loss 0/6064: 0.0006968975067138672\n",
      "Train loss 2000/6064: 0.13886871166910783\n",
      "Train loss 4000/6064: 0.15428132787849627\n",
      "Train loss 6000/6064: 0.15295466681834638\n",
      "Epoch: [9/10],  Train acc: 92.64511873350924, Validation Acc: 87.0545073375262, Val auc: 0.9274378211129131, Val precision: 0.9096176129779838, Val recall: 0.8228511530398323, Val f1: 0.8640616400660429\n",
      "Train loss 0/6064: 0.07923579216003418\n",
      "Train loss 2000/6064: 0.14730583209505324\n",
      "Train loss 4000/6064: 0.14506505027886363\n",
      "Train loss 6000/6064: 0.14967724069061528\n",
      "Epoch: [10/10],  Train acc: 95.15171503957784, Validation Acc: 88.62683438155136, Val auc: 0.9518852541873783, Val precision: 0.9126539753639418, Val recall: 0.8542976939203354, Val f1: 0.8825121819166215\n"
     ]
    }
   ],
   "source": [
    "val_auc, model_sd = train_model(model, optimizer, criterion, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0/6064: 0.0037026405334472656\n",
      "Train loss 2000/6064: 0.1481317178211708\n",
      "Train loss 4000/6064: 0.15125344465357515\n",
      "Train loss 6000/6064: 0.1447324618669852\n",
      "Epoch: [1/10],  Train acc: 95.05277044854881, Validation Acc: 87.78825995807128, Val auc: 0.9405603241784564, Val precision: 0.8645096056622852, Val recall: 0.8962264150943396, Val f1: 0.8800823468862583\n",
      "Train loss 0/6064: 0.0010535717010498047\n",
      "Train loss 2000/6064: 0.15852817967496832\n",
      "Train loss 4000/6064: 0.15080937300733077\n",
      "Train loss 6000/6064: 0.15301303744037992\n",
      "Epoch: [2/10],  Train acc: 94.72295514511873, Validation Acc: 87.99790356394129, Val auc: 0.9502865568784639, Val precision: 0.849566055930569, Val recall: 0.9234800838574424, Val f1: 0.8849824208940232\n",
      "Train loss 0/6064: 0.03731119632720947\n",
      "Train loss 2000/6064: 0.1300439710530086\n",
      "Train loss 4000/6064: 0.14391839028268835\n",
      "Train loss 6000/6064: 0.15029984057555495\n",
      "Epoch: [3/10],  Train acc: 95.15171503957784, Validation Acc: 87.68343815513627, Val auc: 0.9463683750203271, Val precision: 0.8788198103266597, Val recall: 0.8742138364779874, Val f1: 0.8765107724645298\n",
      "Train loss 0/6064: 0.10217010974884033\n",
      "Train loss 2000/6064: 0.1480882727819821\n",
      "Train loss 4000/6064: 0.1531078646001861\n",
      "Train loss 6000/6064: 0.15266639066227197\n",
      "Epoch: [4/10],  Train acc: 95.21767810026385, Validation Acc: 88.46960167714884, Val auc: 0.9461112649376563, Val precision: 0.8855042016806722, Val recall: 0.8836477987421384, Val f1: 0.8845750262329485\n",
      "Train loss 0/6064: 0.31987959146499634\n",
      "Train loss 2000/6064: 0.15120929981517173\n",
      "Train loss 4000/6064: 0.1365353378451666\n",
      "Train loss 6000/6064: 0.1445565789098681\n",
      "Epoch: [5/10],  Train acc: 95.74538258575198, Validation Acc: 87.26415094339623, Val auc: 0.9443840125874066, Val precision: 0.8972067039106145, Val recall: 0.8417190775681341, Val f1: 0.8685776095186587\n",
      "Train loss 0/6064: 0.049658775329589844\n",
      "Train loss 2000/6064: 0.12014090288048801\n",
      "Train loss 4000/6064: 0.13525809137948125\n",
      "Train loss 6000/6064: 0.13715299051158766\n",
      "Epoch: [6/10],  Train acc: 95.39907651715039, Validation Acc: 87.84067085953879, Val auc: 0.9509897639421788, Val precision: 0.8792016806722689, Val recall: 0.8773584905660378, Val f1: 0.8782791185729277\n",
      "Train loss 0/6064: 0.06907534599304199\n",
      "Train loss 2000/6064: 0.12805015301478023\n",
      "Train loss 4000/6064: 0.13166058817317383\n",
      "Train loss 6000/6064: 0.13894946748784057\n",
      "Epoch: [7/10],  Train acc: 95.79485488126649, Validation Acc: 89.41299790356395, Val auc: 0.9494064492877831, Val precision: 0.8844580777096115, Val recall: 0.9067085953878407, Val f1: 0.8954451345755694\n",
      "Train loss 0/6064: 0.00037550926208496094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e2ea51b3bd09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_sd_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-b83e2fa7409e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mloss_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/kae358/miniconda2/envs/NLP/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/kae358/miniconda2/envs/NLP/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_auc, model_sd_2 = train_model(model, optimizer, criterion, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>n_features</th>\n",
       "      <th>n_hidden</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.119827</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.942526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.166065</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.929572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.268502</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.928375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.208548</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.927575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.109671</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.926771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.279322</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.687629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.209043</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.685514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.250164</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.682559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.261245</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.649936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.222664</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.509225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  n_features  n_hidden  hidden_size   dropout  learning_rate  \\\n",
       "38          38         7.0       9.0         50.0  0.119827       0.000892   \n",
       "57          57         7.0       5.0         42.0  0.166065       0.000556   \n",
       "43          43         7.0       5.0         62.0  0.268502       0.000448   \n",
       "17          17         7.0       6.0         91.0  0.208548       0.000272   \n",
       "4            4         7.0       3.0         53.0  0.109671       0.000706   \n",
       "..         ...         ...       ...          ...       ...            ...   \n",
       "42          42         7.0       3.0         36.0  0.279322       0.000017   \n",
       "64          64         7.0       4.0          9.0  0.209043       0.000034   \n",
       "50          50         7.0      15.0         11.0  0.250164       0.000019   \n",
       "99          99         7.0       5.0          6.0  0.261245       0.000024   \n",
       "2            2         7.0       7.0          5.0  0.222664       0.000016   \n",
       "\n",
       "    val_acc   val_auc  \n",
       "38      NaN  0.942526  \n",
       "57      NaN  0.929572  \n",
       "43      NaN  0.928375  \n",
       "17      NaN  0.927575  \n",
       "4       NaN  0.926771  \n",
       "..      ...       ...  \n",
       "42      NaN  0.687629  \n",
       "64      NaN  0.685514  \n",
       "50      NaN  0.682559  \n",
       "99      NaN  0.649936  \n",
       "2       NaN  0.509225  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_search(100, 10, \"results_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"options\": options, \"model\": model_sd}\n",
    "torch.save(d, \"best_model.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load(\"best_model.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SeparateEmbeddings(d[\"options\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeparateEmbeddings(\n",
       "  (hidden_0): Linear(in_features=14, out_features=50, bias=True)\n",
       "  (hidden_1): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (hidden_2): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (hidden_3): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (hidden_4): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (hidden_5): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (hidden_6): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (hidden_7): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (hidden_8): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (projection): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(d[\"model\"])\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88.52201257861635,\n",
       " 0.9540629985628206,\n",
       " 0.9042904290429042,\n",
       " 0.8616352201257862,\n",
       " 0.8824476650563606)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
